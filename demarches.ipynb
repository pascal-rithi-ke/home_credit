{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour remplir la mission 1 :\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "# Connexion à la base de données\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "                       .format(user=\"303869\",\n",
    "                               pw=\"7852xawJBTjDhTc\",\n",
    "                               host=\"mysql-credit-scoring-ipssi.alwaysdata.net\",\n",
    "                               db=\"credit-scoring-ipssi_home-credit\"))\n",
    "\n",
    "dossier = os.getcwd()\n",
    "fichiers_csv = [fichier for fichier in os.listdir(dossier) if fichier.endswith(\".csv\")]\n",
    "\n",
    "for fichier in fichiers_csv:\n",
    "    df = pd.read_csv(fichier)\n",
    "    print(fichier, \"lu\")\n",
    "    df.to_sql(con=engine, name=fichier[:-4], if_exists=\"replace\", index=False, chunksize=1000)\n",
    "    print(fichier, \"importé\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour remplir la mission 2:\n",
    "\n",
    "```python\n",
    "from flask import Flask,render_template, request, jsonify\n",
    "from flask_mysqldb import MySQL\n",
    "\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['MYSQL_HOST'] = os.getenv('host')\n",
    "app.config['MYSQL_USER'] = os.getenv('user')\n",
    "app.config['MYSQL_PASSWORD'] = os.getenv('pw')\n",
    "app.config['MYSQL_DB'] = os.getenv('db')\n",
    "\n",
    "mysql = MySQL(app)\n",
    "\n",
    "if mysql:\n",
    "    print(\"Connection réussie!\")\n",
    "else:\n",
    "    print (\"Connection échouée\")\n",
    "    \n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    # Création curseur\n",
    "    cursor = mysql.connection.cursor()\n",
    "    cursor.execute(\"SELECT * FROM credit_card_balance WHERE SK_ID_PREV LIMIT 50;\")\n",
    "    results = cursor.fetchall()\n",
    "    noms_colonnes = [col[0] for col in cursor.description]\n",
    "\n",
    "    # Création du dictionnaire des résultats\n",
    "    resultat = {}\n",
    "    for i, row in enumerate(results):\n",
    "        # Création du dictionnaire pour chaque ligne de la table\n",
    "        ligne = {}\n",
    "        for j, col in enumerate(row):\n",
    "            ligne[noms_colonnes[j]] = col\n",
    "        resultat[str(i)] = ligne\n",
    "\n",
    "    # Tri des éléments du dictionnaire par ordre croissant\n",
    "    resultat_trie = {}\n",
    "    for key, value in sorted(resultat.items()):\n",
    "        resultat_trie[key] = value\n",
    "    return jsonify({'resultat': resultat_trie})\n",
    "\n",
    "@app.route(\"/client/<id>\")\n",
    "def getClient(id):\n",
    "    cursor = mysql.connection.cursor()\n",
    "    cursor.execute(\"SELECT * FROM MainTable WHERE SK_ID_CURR = %s\", (id,))\n",
    "    results = cursor.fetchall()\n",
    "    noms_colonnes = [col[0] for col in cursor.description]\n",
    "    cursor.close()\n",
    "    if len(results) == 0:\n",
    "        return f\"Aucun client trouvé avec l'ID {id}.\", 404\n",
    "    else:\n",
    "        # Création du dictionnaire des résultats\n",
    "        resultat = {}\n",
    "        for i, row in enumerate(results):\n",
    "            # Création du dictionnaire pour chaque ligne de la table\n",
    "            ligne = {}\n",
    "            for j, col in enumerate(row):\n",
    "                ligne[noms_colonnes[j]] = col\n",
    "            resultat[str(i)] = ligne\n",
    "\n",
    "        # Tri des éléments du dictionnaire par ordre croissant\n",
    "        resultat_trie = {}\n",
    "        for key, value in sorted(resultat.items()):\n",
    "            resultat_trie[key] = value\n",
    "        return jsonify({'resultat': resultat_trie})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.debug = True\n",
    "    app.run(host='localhost', port=5000)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Machine Learning\n",
    "\n",
    "from flask import Flask,render_template, request, jsonify\n",
    "from flask_mysqldb import MySQL\n",
    "from flask_cors import CORS\n",
    "\n",
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "# Training data\n",
    "app_train = pd.read_csv('application_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()\n",
    "# pd.DataFrame.to_json(app_train.head())\n",
    "# print(pd.DataFrame.to_json(app_train.head()))\n",
    "\n",
    "# Testing data features\n",
    "app_test = pd.read_csv('application_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "app_test.head()\n",
    "\n",
    "app_train['TARGET'].value_counts()\n",
    "\n",
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "    \n",
    "    # Missing values statistics\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)\n",
    "\n",
    "# Number of each type of column\n",
    "app_train.dtypes.value_counts()\n",
    "\n",
    "# Number of unique classes in each object column\n",
    "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(app_train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(app_train[col])\n",
    "            # Transform both training and testing data\n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            app_test[col] = le.transform(app_test[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)\n",
    "\n",
    "# one-hot encoding of categorical variables\n",
    "app_train = pd.get_dummies(app_train)\n",
    "app_test = pd.get_dummies(app_test)\n",
    "\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)\n",
    "\n",
    "train_labels = app_train['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target back in\n",
    "app_train['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)\n",
    "\n",
    "(app_train['DAYS_BIRTH'] / -365).describe()\n",
    "app_train['DAYS_EMPLOYED'].describe()\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment');\n",
    "\n",
    "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
    "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n",
    "print('There are %d anomalous days of employment' % len(anom))\n",
    "\n",
    "# Create an anomalous flag column\n",
    "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment');\n",
    "\n",
    "app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "print('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))\n",
    "\n",
    "# Find correlations with the target and sort\n",
    "correlations = app_train.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(15))\n",
    "\n",
    "# Find the correlation of the positive days since birth and target\n",
    "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])\n",
    "\n",
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\n",
    "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');\n",
    "\n",
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
    "\n",
    "# Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
    "age_data.head(10)\n",
    "\n",
    "# Group by the bin and calculate averages\n",
    "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group');\n",
    "\n",
    "# Extract the EXT_SOURCE variables and show correlations\n",
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');\n",
    "\n",
    "plt.figure(figsize = (10, 12))\n",
    "\n",
    "# iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)\n",
    "\n",
    "# Copy the data for plotting\n",
    "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
    "\n",
    "# Add in the age of the client in years\n",
    "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "# Drop na values and limit to first 100000 rows\n",
    "plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "# Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n",
    "                    hue = 'TARGET', \n",
    "                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha = 0.2)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
